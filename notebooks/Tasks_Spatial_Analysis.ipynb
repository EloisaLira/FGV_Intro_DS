{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Science\n",
    "\n",
    "### Data Science Tasks: Geographical and Spatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install plotly-geo geopandas shapely pyshp html5lib\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import twitter\n",
    "import nltk\n",
    "import re\n",
    "import networkx as nx\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import webbrowser\n",
    "import codecs\n",
    "import Levenshtein\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from itertools import chain\n",
    "from itertools import cycle\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nominatim import Nominatim\n",
    "#from mpl_toolkits.basemap import Basemap   #http://matplotlib.org/basemap/api/basemap_api.html\n",
    "from matplotlib import cm\n",
    "import geopy.geocoders as gg\n",
    "\n",
    "\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from IPython.display import Image, HTML, IFrame, FileLink, FileLinks #needed to render in notebook\n",
    "from IPython.core.display import display\n",
    "\n",
    "import pydotplus #Install http://www.graphviz.org/ & Instal https://pypi.python.org/pypi/pydotplus\n",
    "\n",
    "%matplotlib inline\n",
    "# Set default figure size for this notebook\n",
    "plt.rcParams['figure.figsize'] = (16.0, 12.8)\n",
    "#plt.switch_backend('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the path to the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = \"../templates/\"\n",
    "outputs = \"../outputs/\"\n",
    "\n",
    "dotfile = \"graph_retweet.dot\"\n",
    "pngfile = \"graph_retweet.png\"\n",
    "protofile = \"graph_retweet.html\"\n",
    "tweetsfile = \"Tweets_dump.txt\"\n",
    "template_proto = 'template_protoviz.html'\n",
    "\n",
    "pathdotfile = os.path.join(outputs,dotfile)\n",
    "pathpngfile = os.path.join(outputs,pngfile)\n",
    "pathprotofile = os.path.join(outputs,protofile)\n",
    "pathtweetsfile = os.path.join(outputs,tweetsfile)\n",
    "pathtemplate = os.path.join(templates,template_proto)\n",
    "\n",
    "stoplist_en = nltk.corpus.stopwords.words('english')\n",
    "stoplist_pt = nltk.corpus.stopwords.words('portuguese')\n",
    "ignorewords = stoplist_en + stoplist_pt + ['',' ','-','rt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using geographical resources within Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.python.org/pypi/geopy  \n",
    "#https://www.mapsmarker.com/kb/user-guide/how-mapquest-api/\n",
    "#https://geopy.readthedocs.io/en/stable/#openmapquest\n",
    "\n",
    "#geolocator = gg.OpenMapQuest(api_key=OMQapikey)\n",
    "#geolocator = gg.GoogleV3()\n",
    "\n",
    "geolocator = gg.Nominatim(user_agent=\"test_application\") #https://operations.osmfoundation.org/policies/nominatim/\n",
    "#location = geolocator.reverse(\"52.509669, 13.376294\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To geolocate a query to an address and coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rua Dona Mariana, Botafogo, Zona Sul do Rio de Janeiro, Rio de Janeiro, Microrregião Rio de Janeiro, Região Metropolitana do Rio de Janeiro, RJ, Região Sudeste, 22280-020, Brasil\n",
      "(-22.9530169, -43.1885829)\n",
      "{'place_id': 74655297, 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright', 'osm_type': 'way', 'osm_id': 14519270, 'boundingbox': ['-22.9566891', '-22.9497369', '-43.1896942', '-43.1872597'], 'lat': '-22.9530169', 'lon': '-43.1885829', 'display_name': 'Rua Dona Mariana, Botafogo, Zona Sul do Rio de Janeiro, Rio de Janeiro, Microrregião Rio de Janeiro, Região Metropolitana do Rio de Janeiro, RJ, Região Sudeste, 22280-020, Brasil', 'class': 'highway', 'type': 'residential', 'importance': 0.41000000000000003}\n"
     ]
    }
   ],
   "source": [
    "logradouro = \"Dona Mariana, Botafogo\"\n",
    "location = geolocator.geocode(logradouro)\n",
    "print(location.address)\n",
    "address = location.address\n",
    "print((location.latitude, location.longitude))\n",
    "latitude, longitude = location.latitude, location.longitude\n",
    "print(location.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backwerk, Potsdamer Platz, Tiergarten, Mitte, Berlin, 10785, Deutschland\n",
      "(52.50958575, 13.3762845319028)\n",
      "{'place_id': 176241040, 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. https://osm.org/copyright', 'osm_type': 'way', 'osm_id': 464904422, 'lat': '52.50958575', 'lon': '13.3762845319028', 'display_name': 'Backwerk, Potsdamer Platz, Tiergarten, Mitte, Berlin, 10785, Deutschland', 'address': {'bakery': 'Backwerk', 'footway': 'Potsdamer Platz', 'suburb': 'Tiergarten', 'city_district': 'Mitte', 'state': 'Berlin', 'postcode': '10785', 'country': 'Deutschland', 'country_code': 'de'}, 'boundingbox': ['52.5095298', '52.5096533', '13.3762456', '13.3763382']}\n"
     ]
    }
   ],
   "source": [
    "location = geolocator.reverse('52.509669, 13.376294')\n",
    "print(location.address)\n",
    "print((location.latitude, location.longitude))\n",
    "print(location.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7755102040816326"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Measuring editing distances between names:\n",
    "Levenshtein.ratio('Dona Mariano, Botafoga', 'Rua Dona Mariana - Botafogo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800px\"\n",
       "            height=\"600px\"\n",
       "            src=\"http://maps.google.com/maps?q=Rua Dona Mariana, Botafogo, Zona Sul do Rio de Janeiro, Rio de Janeiro, Microrregião Rio de Janeiro, Região Metropolitana do Rio de Janeiro, RJ, Região Sudeste, 22280-020, Brasil&loc:-22.9530169+-43.1885829&z=17&t=k&output=embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7feba2807208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://developers.google.com/maps/documentation/staticmaps/\n",
    "#http://stackoverflow.com/questions/2660201/what-parameters-should-i-use-in-a-google-maps-url-to-go-to-a-lat-lon/9919251#9919251\n",
    "#m – normal map k – satellite h – hybrid p – terrain\n",
    "\n",
    "def gmap(address,lat,lon,zoom=15,tmap='m'):\n",
    "    # Google Maps URL template for an iframe\n",
    "    google_maps_url = 'http://maps.google.com/maps?q={0}&loc:{1}+{2}&z={3}&t={4}&output=embed'.format(address,\n",
    "                                                                                                     lat,\n",
    "                                                                                                     lon,\n",
    "                                                                                                     zoom,\n",
    "                                                                                                     tmap,)\n",
    "    display(IFrame(google_maps_url, '800px', '600px'))\n",
    "    \n",
    "gmap(address, latitude, longitude,17,'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the address corresponding to a set of coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anexo - Senado Federal, N1, SAUS, Asa Sul, Plano Piloto, Região Integrada de Desenvolvimento do Distrito Federal e Entorno, DF, Região Centro-Oeste, 70040906, Brasil\n",
      "(-15.79814865, -47.8641880520898)\n"
     ]
    }
   ],
   "source": [
    "addresses = geolocator.reverse('-15.798,-47.865')\n",
    "for address in addresses:\n",
    "    print(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Plotly\n",
    "inspired by [this](https://towardsdatascience.com/an-introduction-to-geographical-data-visualization-3486959cd4b8) post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict (\n",
    "    type = 'choropleth',\n",
    "    locations = ['China','Canada','Brazil'],\n",
    "    locationmode='country names',\n",
    "    #colorscale = ['Viridis'],\n",
    "    z=[10,20,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map = go.Figure(data=[data])\n",
    "py.offline.plot(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data on [World Hapiness](https://www.kaggle.com/unsdsn/world-happiness#2017.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/CSVs/happiness2017.csv')\n",
    "\n",
    "data = dict (\n",
    "    type = 'choropleth',\n",
    "    locations = df['Country'],\n",
    "    locationmode='country names',\n",
    "    #colorscale = ['Viridis'],\n",
    "    z=df['Happiness.Score'])\n",
    "\n",
    "map = go.Figure(data=[data])\n",
    "py.offline.plot(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dict (\n",
    "    type = 'choropleth',\n",
    "    locations = ['AZ','CA','VT'],\n",
    "    locationmode='USA-states',\n",
    "    #colorscale = ['Viridis'],\n",
    "    z=[10,20,30])\n",
    "\n",
    "lyt = dict(geo=dict(scope='usa'))\n",
    "map = go.Figure(data=[data], layout = lyt)\n",
    "py.offline.plot(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data on [US Employment](https://raw.githubusercontent.com/plotly/datasets/master/laucnty16.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../datasets/CSVs/USunenployment2016.csv')\n",
    "df ['FIPS'] = df['State FIPS Code'].apply(lambda x: str(x).zfill(2)) + df['County FIPS Code'].apply(lambda x: str(x).zfill(3))\n",
    "fips = df['FIPS'].tolist()\n",
    "values = df['Unemployment Rate (%)'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorscale=[\"#f7fbff\",\"#ebf3fb\",\"#deebf7\",\"#d2e3f3\",\"#c6dbef\",\"#b3d2e9\",\"#9ecae1\",\n",
    "            \"#85bcdb\",\"#6baed6\",\"#57a0ce\",\"#4292c6\",\"#3082be\",\"#2171b5\",\"#1361a9\",\n",
    "            \"#08519c\",\"#0b4083\",\"#08306b\"]\n",
    "endpts = list(np.linspace(1, 12, len(colorscale) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsouza/Documents/envs/python_env/lib/python3.6/site-packages/pandas/core/frame.py:7138: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = ff.create_choropleth(fips=fips, values=values, colorscale= colorscale, binning_endpoints=endpts)\n",
    "py.offline.plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python and QGIS for geospatial visualizations - a Case Study  \n",
    "https://www.airpair.com/python/posts/using-python-and-qgis-for-geospatial-visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://www.nuforc.org/webreports/\"\n",
    "index_url = \"http://www.nuforc.org/webreports/ndxevent.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_cast_as_dt(dateStr, fmt):\n",
    "    try:\n",
    "        datetime.strptime(dateStr, fmt)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def parse_dt(dateStr):\n",
    "    # the data in the website comes in two different formats, try both \n",
    "    for fmt in [\"%m/%d/%y %H:%M\", \"%m/%d/%y\"]:\n",
    "        try:\n",
    "            return datetime.strptime(dateStr, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "def get_data_from_url(url):\n",
    "    print(\"Processing {}\".format(url))\n",
    "    data = []\n",
    "    source = BeautifulSoup(urllib.request.urlopen(url), \"html5lib\")\n",
    "    for row in source('tr'):\n",
    "        if not row('td'):\n",
    "            continue # header row\n",
    "        row_data = row('td')\n",
    "        # parse the datetime from the string\n",
    "        date_time = parse_dt(row_data[0].text)\n",
    "        city = row_data[1].text\n",
    "        state = row_data[2].text\n",
    "        shape = row_data[3].text\n",
    "        duration = row_data[4].text\n",
    "        data.append((date_time, city, state, shape, duration))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-225660d4c2ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get the index page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mraw_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_page\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html5lib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# get all the links in the index page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfunc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/envs/python_env/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m        \u001b[0;31m# It's a file-type object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         elif len(markup) <= 256 and (\n\u001b[1;32m    193\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34mb'<'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get the index page\n",
    "raw_page = urllib.request.urlopen(index_url)\n",
    "source = BeautifulSoup(raw_page, \"html5lib\")\n",
    "# get all the links in the index page\n",
    "func1 = lambda x: (x.text, base_url + x['href'])\n",
    "monthly_urls = list(map(func1,source('a')))\n",
    "# get  the last 12 links that have a text like 06/2015\n",
    "func2 = lambda x: can_cast_as_dt(x[0], \"%m/%Y\")\n",
    "last_year_urls = filter(func2, monthly_urls[0:13]) \n",
    "# extract the data from each monthly page and flatten the lists of tuples\n",
    "last_year_ufos = list(chain(*map(lambda x: get_data_from_url(x[1]), last_year_urls)))\n",
    "# initialize a pandas DataFrame with the list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufos_df = pd.DataFrame(last_year_ufos, columns=[\"start\",\"city\",\"state\",\"shape\",\"duration_description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufos_df.replace(to_replace='', value=np.nan, inplace=True, limit=None, regex=False, method='pad', axis=None)\n",
    "ufos_df = ufos_df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufos_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function that infers the duration from the text \n",
    "def infer_duration_in_seconds(text):\n",
    "    # try different regexps to extract the total seconds\n",
    "    text = text.replace('<','')\n",
    "    text = text.replace('>','')\n",
    "    text = text.replace('?','')\n",
    "    text = text.replace('+','')\n",
    "    text = text.replace('~','')\n",
    "    metric_text = [\"second\",\"s\",\"Second\",\"segundo\",\"minute\",\"m\",\"min\",\"Minute\",\"hour\",\"h\",\"Hour\",'Currently']\n",
    "    metric_seconds = [1,1,1,1,60,60,60,3600,3600,3600,10]\n",
    "    for metric,mult in zip(metric_text, metric_seconds):\n",
    "        regex = \"\\s*(\\d+)\\+?\\s*{}s?\".format(metric)\n",
    "        res = re.findall(regex,text)\n",
    "        if len(res)>0:\n",
    "            return int(float(res[0]) * mult)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the duration in seconds\n",
    "ufos_df[\"duration_secs\"] = ufos_df[\"duration_description\"].apply(infer_duration_in_seconds)\n",
    "\n",
    "# now we can infer the end time of the UFO sighting as well\n",
    "# which will be useful for the animation later\n",
    "ufos_df[\"end\"] = ufos_df.apply(lambda x:x[\"start\"] + timedelta(seconds=x[\"duration_secs\"]),axis=1)\n",
    "ufos_df = ufos_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufos_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://wiki.openstreetmap.org/wiki/Nominatim_usage_policy\n",
    "# https://github.com/twain47/Nominatim/blob/master/docs/Installation.md\n",
    "geolocator = Nominatim()\n",
    "\n",
    "geolocator.query('Newport News')\n",
    "#geolocator.query(\"Houston, TX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Be careful with too many queries made to the server in a short period of time\n",
    "\n",
    "ufos_df[\"lat\"] = 0\n",
    "ufos_df[\"lon\"] = 0\n",
    "for i in range(len(ufos_df[0:10])):\n",
    "    try:\n",
    "        resp_json = geolocator.query(ufos_df['city'][i])\n",
    "        ufos_df[\"lat\"][i] = resp_json[0]['lat']\n",
    "        ufos_df[\"lon\"][i] = resp_json[0]['lon']\n",
    "    except:\n",
    "        ufos_df[\"lat\"][i] = 0\n",
    "        ufos_df[\"lat\"][i] = 0\n",
    "    time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/17098654/how-to-store-data-frame-using-pandas-python\n",
    "ufos_df.to_pickle(os.path.join(outputs,'ufos_df.pkl'))\n",
    "ufos_df = pd.read_pickle(os.path.join(outputs,'ufos_df.pkl'))\n",
    "ufos_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: dropna will drop any columns with None values, which is desirable\n",
    "ufos_df[[\"start\",\"end\",\"lon\",\"lat\",\"shape\"]].dropna().to_csv(os.path.join(outputs,'ufo_data.csv'),\n",
    "                                                             index=False, \n",
    "                                                             encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using geographical resources for twitter Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/bear/python-twitter (before was http://code.google.com/p/python-twitter/)  \n",
    "https://dev.twitter.com/docs  \n",
    "\n",
    "Twitter API Keys  \n",
    "Please generate yours...  \n",
    "Go to http://twitter.com/apps/new to create an app and get these items  \n",
    "See https://dev.twitter.com/docs/auth/oauth for more information on Twitter's OAuth implementation  \n",
    "https://dev.twitter.com/rest/reference/get/account/verify_credentials  \n",
    "https://dev.twitter.com/docs/auth/oauth  \n",
    "https://dev.twitter.com/apps/new  \n",
    "\n",
    "Inspiration: http://onemilliontweetmap.com/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitter_tokens.txt', 'r') as twitter_tokens:\n",
    "    tokens = twitter_tokens.read().split(',')\n",
    "consumer_key = tokens[0].strip()\n",
    "consumer_secret = tokens[1].strip()\n",
    "access_token = tokens[2].strip()\n",
    "access_token_secret = tokens[3].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acessing Twitter (with or without authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api = twitter.Api() # Accessing with no authentication\n",
    "api = twitter.Api(consumer_key, consumer_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='http://mike.teczno.com/img/raffi-krikorian-map-of-a-tweet.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent (random) public messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgpublicas = api.GetStreamSample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    tweet = next(msgpublicas)\n",
    "    if 'text' in tweet:\n",
    "        print(u'{}\\n'.format(tweet['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    tweet = next(msgpublicas)\n",
    "    if 'user' in tweet:\n",
    "        break\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent messages from an user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msguser = api.GetUserTimeline(tweet['user']['id'])\n",
    "print([s.text for s in msguser])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent messages from the authenticated user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msguser = api.GetUserTimeline('29959702')\n",
    "print([s.text for s in msguser])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After authentication, more options are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userfollow = api.GetFriends()\n",
    "print([u.name for u in userfollow])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for a term in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_term(termo, pages, results):\n",
    "    '''Search and return tweets on a subject (5 pages of 100 results each)\n",
    "    Save results in a file defined in \"pathtweetsfile\" '''\n",
    "    search_results = []\n",
    "    tweets = []\n",
    "    tweets_txt = []\n",
    "    tweets_words = []\n",
    "    names = []\n",
    "    last = api.GetSearch(term=termo, count=1)\n",
    "    search_results.append(last)\n",
    "    list_ids = []\n",
    "    list_ids.append(last[0].id)\n",
    "    for i in range(pages):\n",
    "        id_last = last[0].id\n",
    "        new_tweets = api.GetSearch(term=termo, count=results, max_id=min(list_ids))\n",
    "        for i in range(len(new_tweets)):\n",
    "            list_ids.append(new_tweets[i].id)\n",
    "        search_results.append(new_tweets)\n",
    "    for i in range(len(search_results)):\n",
    "        for j in range(len(search_results[i])):\n",
    "            tweets.append(search_results[i][j])\n",
    "    tweets_txt += [tweet.text.split(' ') for tweet in tweets]\n",
    "    for i in range(len(tweets)):\n",
    "        tweets_words += [word.lower().strip(':&$!?') for word in tweets_txt[i]]\n",
    "    for i in range(len(tweets)): \n",
    "        names += [word.strip(':&$!?') for word in tweets_txt[i] if word.istitle() and len(word) > 2]\n",
    "    with open(pathtweetsfile,'w') as out:\n",
    "        for tweet in tweets_txt:\n",
    "            out.write('\\n{}'.format(tweet))\n",
    "    return tweets, tweets_txt, tweets_words, names\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(text) / len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_query = 'caminhoneiros'\n",
    "\n",
    "search = api.GetSearch(twitter_query)\n",
    "print([s.text for s in search])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our customized function that retrieves 5 x 100 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, tweets_txt, tweets_words, names = search_for_term(twitter_query,5,10)\n",
    "    \n",
    "print('Word count: {}'.format(len(tweets_words)))\n",
    "print('Repertoire: {}'.format(len(set(tweets_words))))\n",
    "print('Lexical diversity: {}'.format(lexical_diversity(tweets_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(tweets_words)\n",
    "freq_dist.plot(40)\n",
    "freq_dist.plot(40, cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('10 most frequent words')\n",
    "print(freq_dist.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorted list of words')\n",
    "print(sorted(set(tweets_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, without stopwords. See variable \"ignorewords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweets_words = [word for word in tweets_words if word not in ignorewords]\n",
    "    \n",
    "freq_new = nltk.FreqDist(new_tweets_words)    \n",
    "freq_new.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_new.plot(50, cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10 most frequent words')\n",
    "print(freq_new.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorted list of words')\n",
    "print(sorted(set(new_tweets_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_tweets_words.count('greve'))\n",
    "print(freq_new['gasolina']) #same as before\n",
    "print(freq_new.freq('greve')) #relative to the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating small words or words with specific sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_tweets_words = [word for word in new_tweets_words if len(word) > 2]\n",
    "#mediumsized_tweets_words = [word for word in new_tweets_words if len(word) > 2 and len(word) < 9]\n",
    "freq_bigger = nltk.FreqDist(bigger_tweets_words)    \n",
    "freq_bigger.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citacoes = [word for word in tweets_words if '@' in word]\n",
    "#citacoes = [word for word in tweets_words if word.startswith('@')]\n",
    "freq_citacoes = nltk.FreqDist(citacoes)\n",
    "freq_citacoes.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_citacoes.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = [word for word in tweets_words if '#' in word]\n",
    "freq_hashtags = nltk.FreqDist(hashtags)\n",
    "freq_hashtags.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_hashtags.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Frequent words  \n",
    "Can be used with any of the previous lists'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words = [word.lower() for word in new_tweets_words if tweets_words.count(word) > 5]\n",
    "freq_dist2 = nltk.FreqDist(frequent_words)\n",
    "freq_dist2.plot(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_size_words = nltk.FreqDist([len(w) for w in new_tweets_words])\n",
    "freq_size_words.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_size_words.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramas_tweets = nltk.bigrams(new_tweets_words)\n",
    "freqbig = nltk.FreqDist(bigramas_tweets)\n",
    "freqbig.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names (capitalized words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_names = nltk.FreqDist(names)\n",
    "freq_names.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badwords =   ['abandoned','abused','accused','addicted','afraid','aggravated',\n",
    "              'aggressive','alone','angry','anguish','annoyed','anxious','apprehensive',\n",
    "              'argumentative','artificial','ashamed','assaulted','at a loss','at risk',\n",
    "              'atrocious','attacked','avoided','awful','awkward','bad','badgered','baffled',\n",
    "              'banned','barren','beat','beaten down','belittled','berated','betrayed',\n",
    "              'bitched at','bitter','bizzare','blacklisted','blackmailed','blamed','bleak',\n",
    "              'blown away','blur','bored','boring','bossed-around','bothered','bothersome',\n",
    "              'bounded','boxed-in','broken','bruised','brushed-off','bugged','bullied',\n",
    "              'bummed','bummed out','burdened','burdensome','burned','burned-out',\n",
    "              'caged in','careless','chaotic','chased','cheated','cheated on','chicken',\n",
    "              'claustrophobic','clingy','closed','clueless','clumsy','coaxed',\n",
    "              'codependent','coerced','cold','cold-hearted','combative','commanded',\n",
    "              'compared','competitive','compulsive','conceited','concerned',\n",
    "              'condescended to','confined','conflicted','confronted','confused',\n",
    "              'conned','consumed','contemplative','contempt','contentious','controlled',\n",
    "              'convicted','cornered','corralled','cowardly','crabby','cramped','cranky',\n",
    "              'crap','crappy','crazy','creeped out','creepy','critical','criticized',\n",
    "              'cross','crowded','cruddy','crummy','crushed','cut-down','cut-off','cynical',\n",
    "              'damaged','damned','dangerous','dark','dazed','dead','deceived','deep',\n",
    "              'defamed','defeated','defective','defenseless','defensive','defiant',\n",
    "              'deficient','deflated','degraded','dehumanized','dejected','delicate',\n",
    "              'deluded','demanding','demeaned','demented','demoralized','demotivated',\n",
    "              'dependent','depleted','depraved','depressed','deprived','deserted',\n",
    "              'deserving of pain/punishment','desolate','despair','despairing',\n",
    "              'desperate','despicable','despised','destroyed','destructive',\n",
    "              'detached','detest','detestable','detested','devalued','devastated',\n",
    "              'deviant','devoid','diagnosed','dictated to','different','difficult',\n",
    "              'directionless','dirty','disabled','disagreeable','disappointed',\n",
    "              'disappointing','disapproved of','disbelieved','discardable','discarded',\n",
    "              'disconnected','discontent','discouraged','discriminated','disdain',\n",
    "              'disdainful','disempowered','disenchanted','disgraced','disgruntled',\n",
    "              'disgust','disgusted','disheartened','dishonest','dishonorable',\n",
    "              'disillusioned','dislike','disliked','dismal','dismayed','disorganized',\n",
    "              'disoriented','disowned','displeased','disposable','disregarded',\n",
    "              'disrespected','dissatisfied','distant','distracted','distraught',\n",
    "              'distressed','disturbed','dizzy','dominated','doomed','double-crossed',\n",
    "              'doubted','doubtful','down','down and out','down in the dumps',\n",
    "              'downhearted','downtrodden','drained','dramatic','dread','dreadful',\n",
    "              'dreary','dropped','drunk','dry','dumb','dumped','dumped on','duped',\n",
    "              'edgy','egocentric','egotistic','egotistical','elusive','emancipated',\n",
    "              'emasculated','embarrassed','emotional','emotionless','emotionally bankrupt',\n",
    "              'empty','encumbered','endangered','enraged','enslaved','entangled','evaded',\n",
    "              'evasive','evicted','excessive','excluded','exhausted','exploited','exposed',\n",
    "              'fail','failful','fake','false','fear','fearful','fed up','flawed','forced',\n",
    "              'forgetful','forgettable','forgotten','fragile','freaked out','frightened',\n",
    "              'frigid','frustrated','furious','gloomy','glum','gothic','grey','grief','grim',\n",
    "              'gross','grossed-out','grotesque','grouchy','grounded','grumpy','guilt-tripped',\n",
    "              'guilty','harassed','hard','hard-hearted','harmed','hassled','hate','hateful',\n",
    "              'hatred','haunted','heartbroken','heartless','heavy-hearted','helpless',\n",
    "              'hesitant','hideous','hindered','hopeless','horrible','horrified','horror',\n",
    "              'hostile','hot-tempered','humiliated','hung up','hung over','hurried','hurt',\n",
    "              'hysterical','idiot','idiotic','ignorant','ignored','ill','ill-tempered',\n",
    "              'imbalanced','imposed-upon','impotent','imprisoned','impulsive','in the dumps',\n",
    "              'in the way','inactive','inadequate','incapable','incommunicative','incompetent',\n",
    "              'incompatible','incomplete','incorrect','indecisive','indifferent',\n",
    "              'indoctrinated','inebriated','ineffective','inefficient','inferior',\n",
    "              'infuriated','inhibited','inhumane','injured','injusticed','insane',\n",
    "              'insecure','insignificant','insincere','insufficient','insulted',\n",
    "              'intense','interrogated','interrupted','intimidated','intoxicated',\n",
    "              'invalidated','invisible','irrational','irritable','irritated',\n",
    "              'isolated','jaded','jealous','jerked around','joyless','judged',\n",
    "              'kept apart','kept away','kept in','kept out','kept quiet','labeled',\n",
    "              'laughable','laughed at','lazy','leaned on','lectured to','left out',\n",
    "              'let down','lied about','lied to','limited','little','lonely','lonesome',\n",
    "              'longing','lost','lousy','loveless','low','mad','made fun of','man handled',\n",
    "              'manipulated','masochistic','messed with','messed up','messy','miffed',\n",
    "              'miserable','misled','mistaken','mistreated','mistrusted','misunderstood',\n",
    "              'mixed-up','mocked','molested','moody','nagged','needy','negative',\n",
    "              'nervous','neurotic','nonconforming','numb','nuts','nutty','objectified',\n",
    "              'obligated','obsessed','obsessive','obstructed','odd','offended',\n",
    "              'on display','opposed','oppressed','out of place','out of touch',\n",
    "              'over-controlled','over-protected','overwhelmed','pain','panic','paranoid',\n",
    "              'passive','pathetic','pessimistic','petrified','phony','picked on','pissed',\n",
    "              'pissed off','plain','played with','pooped','poor','powerless','pre-judged',\n",
    "              'preached to','preoccupied','predjudiced','pressured','prosecuted',\n",
    "              'provoked','psychopathic','psychotic','pulled apart','pulled back',\n",
    "              'punished','pushed','pushed away','put down','puzzled','quarrelsome',\n",
    "              'queer','questioned','quiet','rage','raped','rattled','regret','rejected',\n",
    "              'resented','resentful','responsible','retarded','revengeful','ridiculed',\n",
    "              'ridiculous','robbed','rotten','sad','sadistic','sarcastic','scared',\n",
    "              'scarred','screwed','screwed over','screwed up','self-centered','self-conscious',\n",
    "              'self-destructive','self-hatred','selfish','sensitive','shouted at','shy',\n",
    "              'singled-out','slow','small','smothered','snapped at','spiteful','stereotyped',\n",
    "              'strange','stressed','stretched','stuck','stupid','submissive','suffering',\n",
    "              'suffocated','suicidal','superficial','suppressed','suspicious','worse','worst'\n",
    "              ,'bankrupcy','jobs','shit','#sob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodwords =  ['Abundant','Accomplished','Achieving','Active','Admirable','Adorable',\n",
    "              'Adventurous','Admired','Affluent','Agreeable','Alert','Aligned','Alive',\n",
    "              'Amazing','Appealing','Appreciate','Artistic','Astounding','Astute',\n",
    "              'Attentive','Attractive','Auspicious','Authentic','Awake','Aware','Awesome',\n",
    "              'Beaming','Beautiful','Better','Best','Blessed','Bliss','Bold','Bright','Brilliant',\n",
    "              'Brisk','Buoyant','Calm','Capable','Centered','Certain','Charming',\n",
    "              'Cheerful','Clear','Clever','Competent','Complete','Confident','Connected',\n",
    "              'Conscious','Considerate','Convenient','Courageous','Creative','Daring',\n",
    "              'Dazzling','Delicious','Delightful','Desirable','Determined','Diligent',\n",
    "              'Discerning','Discover','Dynamic','Eager','Easy','Efficient','Effortless',\n",
    "              'Elegant','Eloquent','Energetic','Endless','Enhancing','Engaging','Enormous'\n",
    "              ,'Enterprising','Enthusiastic','Enticing','Excellent','Exceptional','Exciting'\n",
    "              ,'Experienced','Exquisite','Fabulous','Fair','Far-Sighted','Fascinating',\n",
    "              'Fine','Flattering','Flourishing','Fortunate','Free','Friendly','Fulfilled',\n",
    "              'Fun','Generous','Genuine','Gifted','Glorious','Glowing','Good','Good-Looking',\n",
    "              'Gorgeous','Graceful','Gracious','Grand','Great','Handsome','Happy','Hardy',\n",
    "              'Harmonious','Healed','Healthy','Helpful','Honest','Humorous','Ideal',\n",
    "              'Imaginative','Impressive','Industrious','Ingenious','Innovative','Inspired',\n",
    "              'Intelligent','Interested','Interesting','Intuitive','Inventive','Invincible',\n",
    "              'Inviting','Irresistible','Joyous','Judicious','Keen','Kind','Knowing','Leader',\n",
    "              'Limitless','Lively','Loving','Lucky','Luminous','Magical','Magnificent',\n",
    "              'Marvellous','Masterful','Mighty','Miraculous','Motivated','Natural','Neat',\n",
    "              'Nice','Nurturing','Noble','Optimistic','Outstanding','Passionate','Peaceful',\n",
    "              'Perfect','Persevering','Persistent','Playful','Pleasing','Plentiful','Positive',\n",
    "              'Powerful','Precious','Prepared','Productive','Profound','Prompt','Prosperous',\n",
    "              'Proud','Qualified','Quick','Radiant','Reasonable','Refined','Refreshing',\n",
    "              'Relaxing','Reliable','Remarkable','Resolute','Resourceful','Respected',\n",
    "              'Rewarding','Robust','Safe','Satisfied','Secure','Seductive','Self-Reliant',\n",
    "              'Sensational','Sensible','Sensitive','Serene','Sharing','Skilful','Smart',\n",
    "              'Smashing','Smooth','Sparkling','Spiritual','Splendid','Strong','Stunning',\n",
    "              'Successful','Superb','Swift','Talented','Tenacious','Terrific','Thankful',\n",
    "              'Thrilling','Thriving','Timely','Trusting','Truthful','Ultimate','Unique',\n",
    "              'Valiant','Valuable','Versatile','Vibrant','Victorious','Vigorous','Vivacious',\n",
    "              'Vivid','Warm','Wealthy','Well','Whole','Wise','Wonderful','Worthy','Young',\n",
    "              'Youthful','Zeal','Zest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(count, total):\n",
    "    return 100 * count / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(texto, goodwords, badwords):\n",
    "    '''\n",
    "    Not a sophisticated one, but the main idea is present.\n",
    "    Please read: http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html\n",
    "    '''\n",
    "    goodness = 0\n",
    "    badness = 0    \n",
    "    for word in goodwords:\n",
    "        goodness += percentage(texto.count(word.lower()), len(texto))\n",
    "    for word in badwords:\n",
    "        badness += percentage(texto.count(word.lower()), len(texto))\n",
    "    if badness:\n",
    "        ratio = goodness/float(badness)\n",
    "    print(u'Grau de negatividade: {}'.format(badness))\n",
    "    print(u'Grau de positividade: {}'.format(goodness))\n",
    "    if badness:\n",
    "        print(u'Razão: {}'.format(ratio))\n",
    "    return goodness, badness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(tweets_words, goodwords, badwords);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geographic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.python.org/pypi/basemap/1.0.7  \n",
    "http://matplotlib.org/basemap/  \n",
    "http://matplotlib.org/basemap/users/installing.html  \n",
    "https://github.com/SciTools/Cartopy  \n",
    "https://scitools.org.uk/cartopy/docs/v0.15/installing.html  \n",
    "http://nbviewer.ipython.org/github/ehmatthes/intro_programming/blob/master/notebooks/visualization_earthquakes.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgpublicas = api.GetStreamSample()\n",
    "geo_points = 0\n",
    "lats, lons = [], []\n",
    "users = []\n",
    "while geo_points < 20:\n",
    "    tweet = next(msgpublicas)\n",
    "    if 'coordinates' in tweet.keys() and tweet['coordinates'] != None:\n",
    "        coords = tweet['coordinates']['coordinates']\n",
    "        user = tweet['user']['id']\n",
    "        print(u'Usuário {} nas coordenadas {}'.format(user, [coords[1],coords[0]]))\n",
    "        lons.append(float(coords[0]))\n",
    "        lats.append(float(coords[1]))\n",
    "        users.append(user)\n",
    "        geo_points +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = Basemap(projection='robin', resolution = 'l', area_thresh = 1000.0, lat_0=0, lon_0=0)\n",
    "\n",
    "map.drawcoastlines()\n",
    "map.drawcountries()\n",
    "map.fillcontinents(color = '#FFFFCC') #http://www.w3schools.com/tags/ref_colorpicker.asp\n",
    "map.drawmapboundary()\n",
    "map.drawmeridians(np.arange(0, 360, 30))\n",
    "map.drawparallels(np.arange(-90, 90, 30))\n",
    "\n",
    "x,y = map(lons, lats)\n",
    "map.plot(x, y, 'ro', markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = Basemap(projection='merc', lat_0 = -22, lon_0 = -56.5,resolution = 'h', area_thresh = 0.1, \n",
    "              llcrnrlon=-83.0, llcrnrlat=-57.0, urcrnrlon=-30.0, urcrnrlat=13.0)\n",
    " \n",
    "map.drawcoastlines()\n",
    "map.drawcountries()\n",
    "map.fillcontinents(color = 'coral')\n",
    "map.drawmapboundary()\n",
    "\n",
    "x,y = map(lons, lats)\n",
    "map.plot(x, y, 'bo', markersize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom em uma coordenada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmap(lat,lon,zoom=10):\n",
    "    # Google Maps URL template for an iframe\n",
    "    google_maps_url = \"http://maps.google.com/maps?q={0}+{1}&ie=UTF8&t=h&z={2}&{0},{1}&output=embed\".format(lat,lon,zoom)\n",
    "    display(IFrame(google_maps_url, '800px', '600px'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap(lats[0],lons[0],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs\n",
    "\n",
    "http://networkx.lanl.gov/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rt_origins(tweet):\n",
    "    ''' Regex adapted from \n",
    "    http://stackoverflow.com/questions/655903/python-regular-expression-for-retweets'''\n",
    "    rt_patterns = re.compile(r\"(RT|via)((?:\\b\\W*@\\w+)+)\", re.IGNORECASE)\n",
    "    rt_origins = []\n",
    "    try:\n",
    "        rt_origins += [mention.strip() for mention in rt_patterns.findall(tweet)[0][1].split()]\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "    return [rto.strip(\"@\") for rto in rt_origins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_retweets(tweets):\n",
    "    g = nx.DiGraph()\n",
    "    for tweet in tweets:\n",
    "        rt_origins = get_rt_origins(tweet.text)\n",
    "        if not rt_origins:\n",
    "            continue\n",
    "        for rt_origin in rt_origins:\n",
    "            g.add_edge(rt_origin, tweet.user.screen_name, {'tweet_id': tweet.id})\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_rt = create_graph_retweets(tweets)\n",
    "print(\"Number of nodes is: {}\\n\".format(g_rt.number_of_nodes()))\n",
    "print(\"Number of edges is: {}\\n\".format(g_rt.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_dic = sorted(g_rt.degree().items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = nx.degree(g_rt)\n",
    "plt.plot(sorted(dic.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dotfile(g):\n",
    "    try:\n",
    "        nx.drawing.write_dot(g, pathdotfile)\n",
    "        print >> sys.stderr, 'Graph exported for file: {}'.format(pathdotfile)\n",
    "    except (ImportError, UnicodeEncodeError, AttributeError): \n",
    "        # Este bloco serve para usuarios de windows, que certamente terao problemas\n",
    "        # com o metodo nx.drawing.write_dot. Tambem serve para os casos em que temos\n",
    "        # problemas com o unicode\n",
    "        dot = [u'\"{}\" -> \"{}\" [tweet_id={}]'.format(n1, n2, g[n1][n2]['tweet_id']) for (n1, n2) in g.edges()]\n",
    "        f = codecs.open(pathdotfile, 'w', encoding='utf-8')\n",
    "        f.write('''strict digraph {{}}'''.format(';\\n'.join(dot), ))\n",
    "        f.close()\n",
    "        print(sys.stderr, 'Graph exported for file: {}'.format(pathdotfile))\n",
    "        return f.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dotfile(g_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a dotfile\n",
    "\n",
    "Obs: To generate a png graph from the dotfile, type in the Unix Prompt: \n",
    "'circo -Tpng -Gcharset=latin1 -Ograph_retweet graph_retweet.dot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Chamando um programa externo ao Ipython com o operador !\n",
    "!dot -Tpng ../outputs/graph_retweet.dot -o ../outputs/graph_retweet.png\n",
    "Image(pathpngfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx_labels(g_rt,pos=nx.spring_layout(g_rt),font_size=9)\n",
    "nx.draw(g_rt)\n",
    "#nx.draw_random(g_rt)\n",
    "#nx.draw_circular(g_rt)\n",
    "#nx.draw_spectral(g_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a HTML file that uses javascript for visualizing the graph (needs a template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_protovis_file(g):\n",
    "    '''A visualization alternative is \"protovis\" javascript\n",
    "    It uses the files \"template_protoviz.html and \"protovis-r3.2.js\"\n",
    "    '''\n",
    "    nodes = g.nodes()\n",
    "    indexed_nodes = {}\n",
    "    idx = 0\n",
    "    for n in nodes:\n",
    "        indexed_nodes.update([(n, idx,)])\n",
    "        idx += 1\n",
    "    links = []\n",
    "    for n1, n2 in g.edges():\n",
    "        links.append({'source': indexed_nodes[n2],'target': indexed_nodes[n1]})\n",
    "    json_data = json.dumps({\"nodes\" : [{\"nodeName\" : n} for n in nodes], \"links\" : links}, indent=4)\n",
    "    html = open(pathtemplate).read().format(json_data,)\n",
    "    f = open(pathprotofile, 'w')\n",
    "    f.write(html)\n",
    "    f.close()\n",
    "    print(sys.stderr, 'Graph exported for file: {}'.format(pathprotofile))\n",
    "    return f.name, html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = save_protovis_file(g_rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!chromium ../outputs/graph_retweet.html\n",
    "# http://docs.python.org/library/webbrowser.html \n",
    "\n",
    "webbrowser.open(pathprotofile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
